# ğŸ¯ Ollama Configuration for 16GB VRAM

## âœ… Ollama Test Results

Your Ollama is working perfectly! Here's what we confirmed:

- âœ… **Qwen3 14B model** (9.3GB) - Works great on your 16GB VRAM!
- âœ… **Response time**: ~5.4 seconds for generation
- âœ… **Model loaded**: Successfully loaded 6.7GB
- âœ… **Quality**: Excellent reasoning (see the detailed "thinking" output!)
- ğŸ’° **Cost**: $0.00 (100% local!)

## ğŸ“Š Your VRAM Usage Analysis

With **16GB VRAM**, here's what fits:

| Model | Size | VRAM Needed | Status | Best For |
|-------|------|-------------|--------|----------|
| **qwen3:14b-q4_k_m** | 9.3GB | ~10GB | âœ… **RECOMMENDED** | RBI agent, general use |
| qwen3:30b | 18.5GB | ~19GB | âŒ Too large | N/A |
| **glm-4.6:cloud** | 366B | N/A | âœ… Cloud API | Optional backup |
| **minimax-m2:cloud** | 230B | N/A | âœ… Cloud API | Optional backup |

## ğŸš€ Recommended Setup for YOU

### Primary Model: **Qwen3 14B** (q4_k_m quantization)

**Why this model is perfect for you**:
1. âœ… Fits comfortably in 16GB VRAM
2. âœ… Excellent quality (14.8B parameters)
3. âœ… Good reasoning capabilities (see the "thinking" output!)
4. âœ… Fast enough for backtesting (5-6 seconds per response)
5. âœ… Completely FREE

**Performance stats** (from our test):
```
Response: "OK"
Thinking process: 166 tokens of reasoning
Generation time: 5.4 seconds
Prompt eval: 83ms for 17 tokens
Output eval: 5.4s for 148 tokens
Speed: ~27 tokens/second
```

## ğŸ”§ Configuration for Moon Dev Agents

### Step 1: Add to .env file

Add these lines to your [.env](.env):

```bash
# Ollama Local AI Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:14b-q4_k_m
USE_LOCAL_AI=true  # Prefer local over cloud APIs
```

### Step 2: Create ollama_config.py

Save this as `src/config/ollama_config.py`:

```python
"""
Ollama Configuration for Moon Dev Agents
Use FREE local AI instead of paid APIs
"""
import os
from dotenv import load_dotenv

load_dotenv()

OLLAMA_CONFIG = {
    "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
    "model": os.getenv("OLLAMA_MODEL", "qwen3:14b-q4_k_m"),
    "temperature": 0.7,
    "num_ctx": 4096,  # Context window
}

def get_ollama_url():
    """Get Ollama API URL"""
    return f"{OLLAMA_CONFIG['base_url']}/api/generate"

def get_model_name():
    """Get configured Ollama model"""
    return OLLAMA_CONFIG['model']
```

### Step 3: Using Ollama with LiteLLM

Moon Dev agents use LiteLLM which supports Ollama. Configure it:

```python
import litellm

# Use Ollama instead of cloud APIs
response = litellm.completion(
    model="ollama/qwen3:14b-q4_k_m",
    messages=[{"role": "user", "content": "Explain RSI"}],
    api_base="http://localhost:11434"
)
```

## ğŸ’° Cost Comparison

### Running RBI Agent 100 times:

**Cloud APIs**:
- Claude: ~$15-30
- GPT-4: ~$20-40
- DeepSeek: ~$0.50-1.00

**Your Ollama (Qwen3 14B)**:
- Cost: **$0.00** ğŸ‰
- Only electricity: ~$0.10 for GPU power

**Savings**: ~$15-40 per 100 runs!

## ğŸ¯ Usage Strategy

### When to use each model:

**1. Ollama Qwen3 14B (LOCAL)** - Use for:
- âœ… All testing and experimentation
- âœ… RBI backtesting iterations
- âœ… Research agent
- âœ… Initial strategy development
- âœ… Learning and practice
- **Cost**: $0.00

**2. DeepSeek API** - Use for:
- âš¡ When Ollama is too slow
- âš¡ Need faster responses
- âš¡ Ollama is busy/unavailable
- **Cost**: Very cheap ($0.14-0.28 per 1M tokens)

**3. Claude API** - Use ONLY for:
- ğŸ¯ Final strategy validation
- ğŸ¯ Critical decisions
- ğŸ¯ Production runs
- ğŸ¯ Highest quality needed
- **Cost**: $3-15 per 1M tokens (your limited credits)

## ğŸ§ª Testing Commands

### Test Ollama directly:
```bash
python test_ollama.py
```

### Test via API call:
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:14b-q4_k_m",
  "prompt": "Explain moving average in one sentence",
  "stream": false
}'
```

### Test with Python:
```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "qwen3:14b-q4_k_m",
        "prompt": "Your prompt here",
        "stream": False
    }
)
print(response.json()['response'])
```

## ğŸ”„ Model Switching Guide

### Quick switch between models:

**Edit .env file**:
```bash
# Use local Ollama (FREE)
#OLLAMA_MODEL=qwen3:14b-q4_k_m

# Use cloud backup (if Ollama fails)
#OLLAMA_MODEL=glm-4.6:cloud
```

## ğŸ“ˆ Expected Performance

### Qwen3 14B on 16GB VRAM:

| Task | Speed | Quality | Cost |
|------|-------|---------|------|
| Simple prompts | 3-5s | Excellent | $0 |
| Strategy generation | 10-15s | Very good | $0 |
| Code generation | 15-20s | Good | $0 |
| Complex reasoning | 20-30s | Very good | $0 |

### Tips for faster responses:

1. **Reduce max tokens**: Shorter responses = faster
2. **Lower temperature**: Less creative = faster
3. **Simplify prompts**: Clearer = faster
4. **Batch processing**: Process multiple at once

## ğŸ Bonus: n8n Integration

Since you have n8n running, you can:

1. Create workflow to auto-run backtests
2. Use Ollama via HTTP node
3. Schedule strategy testing overnight
4. Send results to Discord/Telegram

**n8n HTTP Request Node**:
```
Method: POST
URL: http://ollama:11434/api/generate
Body:
{
  "model": "qwen3:14b-q4_k_m",
  "prompt": "{{$json.strategy_idea}}",
  "stream": false
}
```

## ğŸš¨ Troubleshooting

### Ollama not responding?
```bash
docker restart ollama
docker logs ollama
```

### Out of memory?
```bash
# Check GPU memory
nvidia-smi

# If full, restart Ollama
docker restart ollama
```

### Slow responses?
- âœ… Normal for first request (model loading)
- âœ… Subsequent requests should be faster
- âš ï¸ If always slow, check CPU/GPU usage

## ğŸ“š Next Steps

1. âœ… **Ollama tested and working!**
2. â³ Wait for Python packages to finish installing
3. â³ Test Claude API (minimal test)
4. â³ Configure RBI agent for Ollama
5. â³ Run first backtest with FREE local AI!

---

**Bottom line**: Your Ollama setup with Qwen3 14B is **perfect** for your 16GB VRAM and will save you **significant money** on API costs! ğŸ‰
